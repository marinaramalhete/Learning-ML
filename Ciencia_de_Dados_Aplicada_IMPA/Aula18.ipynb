{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudos de Caso - Classificação Multi-classe\n",
    "\n",
    "\n",
    "#### 1 - Importando a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./CSV/Consumer_Complaints.csv')\n",
    "\n",
    "df = df.sample(frac=0.05, random_state=10)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Limpando a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = ['Product', 'Consumer complaint narrative']\n",
    "\n",
    "df = df[col]\n",
    "df = df[pd.notnull(df['Consumer complaint narrative'])]\n",
    "\n",
    "df.columns = ['Product', 'Consumer_complaint_narrative']\n",
    "\n",
    "df['category_id'] = df['Product'].factorize()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_df = df[['Product', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "\n",
    "category_to_id = dict(category_id_df.values)\n",
    "\n",
    "id_to_category = dict(category_id_df[['category_id', 'Product']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Plotando as variáveis categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "df.groupby('Product').Consumer_complaint_narrative.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Mala de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "features = tfidf.fit_transform(df.Consumer_complaint_narrative).toarray()\n",
    "\n",
    "labels = df.category_id\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 - Chi2 para encontrar termos mais relacionados a cada categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "import numpy as np\n",
    "N = 2\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2 = chi2(features, labels == category_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "  print(\"# '{}':\".format(Product))\n",
    "  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 - Treinando o classificador Naive-Bayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Consumer_complaint_narrative'], df['Product'], random_state = 0)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 - Fazendo previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(clf.predict(count_vect.transform([\"This company refuses to provide me verification and validation of debt per my right under the FDCPA. I do not believe this debt is mine.\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(clf.predict(count_vect.transform([\"I am disputing the inaccurate information the Chex-Systems has on my credit report. I initially submitted a police report on XXXX/XXXX/16 and Chex Systems only deleted the items that I mentioned in the letter and not all the items that were actually listed on the police report. In other words they wanted me to say word for word to them what items were fraudulent. The total disregard of the police report and what accounts that it states that are fraudulent. If they just had paid a little closer attention to the police report I would not been in this position now and they would n't have to research once again. I would like the reported information to be removed : XXXX XXXX XXXX\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 - Selecionando o melhor classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "cv_df.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9 - Plotando os resultados do processo de seleção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 - Matriz de confusão SVC linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.Product.values, yticklabels=category_id_df.Product.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11 - Verificação de causa de classificações incorretas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 10:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "      display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Product', 'Consumer_complaint_narrative']])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12 - Chi2 para encontrar termos mais correlacionados com cada categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(features, labels)\n",
    "N = 2\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "  indices = np.argsort(model.coef_[category_id])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n",
    "  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n",
    "  print(\"# '{}':\".format(Product))\n",
    "  print(\"  . Top unigrams:\\n       . {}\".format('\\n       . '.join(unigrams)))\n",
    "  print(\"  . Top bigrams:\\n       . {}\".format('\\n       . '.join(bigrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13 - Reporte de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=df['Product'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudos de Caso - Classificação Multi-label\n",
    "\n",
    "#### 14 - Importando bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df = pd.read_csv(\"./CSV/trainToxic.csv\", encoding = \"ISO-8859-1\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15 - Número de comentários em cada categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_toxic = df.drop(['id', 'comment_text'], axis=1)\n",
    "counts = []\n",
    "categories = list(df_toxic.columns.values)\n",
    "\n",
    "for i in categories:\n",
    "    counts.append((i, df_toxic[i].sum()))\n",
    "    \n",
    "df_stats = pd.DataFrame(counts, columns=['category', 'number_of_comments'])\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16 - Plotando números de comentários por categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_stats.plot(x='category', y='number_of_comments', kind='bar', legend=False, grid=True, figsize=(8, 5))\n",
    "plt.ylabel('# of Occurrences', fontsize=12)\n",
    "plt.xlabel('category', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16 - Quantidade de comentários com rótulos múltiplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowsums = df.iloc[:,2:].sum(axis=1)\n",
    "x=rowsums.value_counts()#plot\n",
    "plt.figure(figsize=(8,5))\n",
    "ax = sns.barplot(x.index, x.values)\n",
    "plt.ylabel('# of Occurrences', fontsize=12)\n",
    "plt.xlabel('# of categories', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17 - Porcentagem de comentários não rotulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[(df['toxic']==0) & (df['severe_toxic']==0) & (df['obscene']==0) & (df['threat']== 0) & (df['insult']==0) & (df['identity_hate']==0)]) / len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18 - Distribuição do número de caractéres nos comentários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = df[df['insult'] == 1].comment_text.str.len()\n",
    "lens.hist(bins = np.arange(0,200,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18 - Há comentários faltando?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19 - Exemplo do primeiro comentário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['comment_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 - Limpeza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "df['comment_text'] = df['comment_text'].map(lambda com : clean_text(com))\n",
    "\n",
    "df['comment_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21 - Separando dados de treinamento e testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
    "X_train = train.comment_text\n",
    "X_test = test.comment_text\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22 - Pipeline UmContraTodos + Naive-Bayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                    fit_prior=True, class_prior=None))),\n",
    "            ])\n",
    "\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # treina\n",
    "    NB_pipeline.fit(X_train, train[category])\n",
    "    # prevê\n",
    "    prediction = NB_pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 23 - Pipeline UmContraTodos + LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=5)),\n",
    "            ])\n",
    "\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # treina\n",
    "    SVC_pipeline.fit(X_train, train[category])\n",
    "    # prevê\n",
    "    prediction = SVC_pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
