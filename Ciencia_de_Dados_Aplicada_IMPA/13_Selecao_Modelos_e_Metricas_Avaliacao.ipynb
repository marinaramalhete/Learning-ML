{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção de Modelos & Métricas de avaliação\n",
    "\n",
    "#### 1 - Bibliotecas, Datasets e Funções de Plotagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "\n",
    "# Base de Dados Frutas\n",
    "\n",
    "fruits = pd.read_table('./CSV/fruit_data_with_colors.txt')\n",
    "\n",
    "feature_names_fruits = ['height', 'width', 'mass', 'color_score']\n",
    "X_fruits = fruits[feature_names_fruits]\n",
    "y_fruits = fruits['fruit_label']\n",
    "target_names_fruits = ['apple', 'mandarin', 'orange', 'lemon']\n",
    "\n",
    "X_fruits_2d = fruits[['height', 'width']]\n",
    "y_fruits_2d = fruits['fruit_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits, y_fruits, random_state=0)\n",
    "\n",
    "# Base de dados sintética para classificação binária simples\n",
    "plt.figure()\n",
    "plt.title('Base de dados sintética para classificação binária simples')\n",
    "X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,\n",
    "                                n_redundant=0, n_informative=2,\n",
    "                                n_clusters_per_class=1, flip_y = 0.1,\n",
    "                                class_sep = 0.5, random_state=0)\n",
    "plt.scatter(X_C2[:, 0], X_C2[:, 1], c=y_C2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Base de dados sintética para classificação complexa\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2, centers = 8,\n",
    "                       cluster_std = 1.3, random_state = 4)\n",
    "y_D2 = y_D2 % 2\n",
    "plt.figure()\n",
    "plt.title('Base de dados sintética para classificação complexa')\n",
    "plt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()\n",
    "\n",
    "#Funções de Plotagem\n",
    "\n",
    "def plot_class_regions_for_classifier_subplot(clf, X, y, X_test, y_test, title, subplot, target_names = None, plot_decision_regions = True):\n",
    "\n",
    "    numClasses = np.amax(y) + 1\n",
    "    color_list_light = ['#FFFFAA', '#EFEFEF', '#AAFFAA', '#AAAAFF']\n",
    "    color_list_bold = ['#EEEE00', '#000000', '#00CC00', '#0000CC']\n",
    "    cmap_light = ListedColormap(color_list_light[0:numClasses])\n",
    "    cmap_bold  = ListedColormap(color_list_bold[0:numClasses])\n",
    "\n",
    "    h = 0.03\n",
    "    k = 0.5\n",
    "    x_plot_adjust = 0.1\n",
    "    y_plot_adjust = 0.1\n",
    "    plot_symbol_size = 50\n",
    "\n",
    "    x_min = X[:, 0].min()\n",
    "    x_max = X[:, 0].max()\n",
    "    y_min = X[:, 1].min()\n",
    "    y_max = X[:, 1].max()\n",
    "    x2, y2 = np.meshgrid(np.arange(x_min-k, x_max+k, h), np.arange(y_min-k, y_max+k, h))\n",
    "\n",
    "    P = clf.predict(np.c_[x2.ravel(), y2.ravel()])\n",
    "    P = P.reshape(x2.shape)\n",
    "\n",
    "    if plot_decision_regions:\n",
    "        subplot.contourf(x2, y2, P, cmap=cmap_light, alpha = 0.8)\n",
    "\n",
    "    subplot.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=plot_symbol_size, edgecolor = 'black')\n",
    "    subplot.set_xlim(x_min - x_plot_adjust, x_max + x_plot_adjust)\n",
    "    subplot.set_ylim(y_min - y_plot_adjust, y_max + y_plot_adjust)\n",
    "\n",
    "    if (X_test is not None):\n",
    "        subplot.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, s=plot_symbol_size, marker='^', edgecolor = 'black')\n",
    "        train_score = clf.score(X, y)\n",
    "        test_score  = clf.score(X_test, y_test)\n",
    "        title = title + \"\\nTreinamento = {:.2f}, Teste = {:.2f}\".format(train_score, test_score)\n",
    "\n",
    "    subplot.set_title(title)\n",
    "\n",
    "    if (target_names is not None):\n",
    "        legend_handles = []\n",
    "        for i in range(0, len(target_names)):\n",
    "            patch = mpatches.Patch(color=color_list_bold[i], label=target_names[i])\n",
    "            legend_handles.append(patch)\n",
    "        subplot.legend(loc=0, handles=legend_handles)\n",
    "\n",
    "\n",
    "def plot_class_regions_for_classifier(clf, X, y, X_test=None, y_test=None, title=None, target_names = None, plot_decision_regions = True):\n",
    "\n",
    "    numClasses = np.amax(y) + 1\n",
    "    color_list_light = ['#FFFFAA', '#EFEFEF', '#AAFFAA', '#AAAAFF']\n",
    "    color_list_bold = ['#EEEE00', '#000000', '#00CC00', '#0000CC']\n",
    "    cmap_light = ListedColormap(color_list_light[0:numClasses])\n",
    "    cmap_bold  = ListedColormap(color_list_bold[0:numClasses])\n",
    "\n",
    "    h = 0.03\n",
    "    k = 0.5\n",
    "    x_plot_adjust = 0.1\n",
    "    y_plot_adjust = 0.1\n",
    "    plot_symbol_size = 50\n",
    "\n",
    "    x_min = X[:, 0].min()\n",
    "    x_max = X[:, 0].max()\n",
    "    y_min = X[:, 1].min()\n",
    "    y_max = X[:, 1].max()\n",
    "    x2, y2 = np.meshgrid(np.arange(x_min-k, x_max+k, h), np.arange(y_min-k, y_max+k, h))\n",
    "\n",
    "    P = clf.predict(np.c_[x2.ravel(), y2.ravel()])\n",
    "    P = P.reshape(x2.shape)\n",
    "    plt.figure()\n",
    "    if plot_decision_regions:\n",
    "        plt.contourf(x2, y2, P, cmap=cmap_light, alpha = 0.8)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=plot_symbol_size, edgecolor = 'black')\n",
    "    plt.xlim(x_min - x_plot_adjust, x_max + x_plot_adjust)\n",
    "    plt.ylim(y_min - y_plot_adjust, y_max + y_plot_adjust)\n",
    "\n",
    "    if (X_test is not None):\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, s=plot_symbol_size, marker='^', edgecolor = 'black')\n",
    "        train_score = clf.score(X, y)\n",
    "        test_score  = clf.score(X_test, y_test)\n",
    "        title = title + \"\\nTreinamento = {:.2f}, Teste = {:.2f}\".format(train_score, test_score)\n",
    "\n",
    "    if (target_names is not None):\n",
    "        legend_handles = []\n",
    "        for i in range(0, len(target_names)):\n",
    "            patch = mpatches.Patch(color=color_list_bold[i], label=target_names[i])\n",
    "            legend_handles.append(patch)\n",
    "        plt.legend(loc=0, handles=legend_handles)\n",
    "\n",
    "    if (title is not None):\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Exemplo de Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "dataset = load_digits()\n",
    "\n",
    "X, y = dataset.data, dataset.target == 1\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "\n",
    "print('Acurácia', cross_val_score(clf, X, y, cv=5))\n",
    "print('AUC', cross_val_score(clf, X, y, cv=5, scoring = 'roc_auc'))\n",
    "print('Recall', cross_val_score(clf, X, y, cv=5, scoring = 'recall'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Exemplo de GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target == 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "clf = SVC(kernel='rbf')\n",
    "grid_values = {'gamma': [0.001, 0.01, 0.05, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Acurácia é o Padrão\n",
    "grid_clf_acc = GridSearchCV(clf, param_grid = grid_values)\n",
    "grid_clf_acc.fit(X_train, y_train)\n",
    "y_decision_fn_scores_acc = grid_clf_acc.decision_function(X_test) \n",
    "\n",
    "print('Melhor parâmetro (max. acurácia): ', grid_clf_acc.best_params_)\n",
    "print('Melhor escore (acurácia): ', grid_clf_acc.best_score_)\n",
    "\n",
    "# AUC\n",
    "grid_clf_auc = GridSearchCV(clf, param_grid = grid_values, scoring = 'roc_auc')\n",
    "grid_clf_auc.fit(X_train, y_train)\n",
    "y_decision_fn_scores_auc = grid_clf_auc.decision_function(X_test) \n",
    "\n",
    "roc_auc_score?\n",
    "\n",
    "print('Conjunto de teste AUC: ', roc_auc_score(y_test, y_decision_fn_scores_auc))\n",
    "print('Melhor parâmetro (max. AUC): ', grid_clf_auc.best_params_)\n",
    "print('Melhor escore (AUC): ', grid_clf_auc.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Lista completa das métricas de avaliação suportadas pelo sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import SCORERS\n",
    "\n",
    "print(sorted(list(SCORERS.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 - Otimizando o classificador usando métricas de desempenho diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target == 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "jitter_delta = 0.25\n",
    "X_twovar_train = X_train[:,[20,59]]+ np.random.rand(X_train.shape[0], 2) - jitter_delta\n",
    "X_twovar_test  = X_test[:,[20,59]] + np.random.rand(X_test.shape[0], 2) - jitter_delta\n",
    "\n",
    "clf = SVC(kernel = 'linear').fit(X_twovar_train, y_train)\n",
    "grid_values = {'class_weight':['balanced', {1:2},{1:3},{1:4},{1:5},{1:10},{1:20},{1:50}]}\n",
    "plt.figure(figsize=(9,6))\n",
    "for i, eval_metric in enumerate(('precision','recall', 'f1','roc_auc')):\n",
    "    grid_clf_custom = GridSearchCV(clf, param_grid=grid_values, scoring=eval_metric)\n",
    "    grid_clf_custom.fit(X_twovar_train, y_train)\n",
    "    print('Melhor parâmetro (max. {0}): {1}'\n",
    "          .format(eval_metric, grid_clf_custom.best_params_))\n",
    "    print('Melhor escore ({0}): {1}'\n",
    "          .format(eval_metric, grid_clf_custom.best_score_))\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    plot_class_regions_for_classifier_subplot(grid_clf_custom, X_twovar_test, y_test, None,\n",
    "                                             None, None,  plt.subplot(2, 2, i+1))\n",
    "    \n",
    "    plt.title(eval_metric+'-SVC')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 - Curva de precisão-recall para classificador SVC - pesos de classe balanceados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target == 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "jitter_delta = 0.25\n",
    "X_twovar_train = X_train[:,[20,59]]+ np.random.rand(X_train.shape[0], 2) - jitter_delta\n",
    "X_twovar_test  = X_test[:,[20,59]] + np.random.rand(X_test.shape[0], 2) - jitter_delta\n",
    "\n",
    "clf = SVC(kernel='linear', class_weight='balanced').fit(X_twovar_train, y_train)\n",
    "\n",
    "y_scores = clf.decision_function(X_twovar_test)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "closest_zero = np.argmin(np.abs(thresholds))\n",
    "closest_zero_p = precision[closest_zero]\n",
    "closest_zero_r = recall[closest_zero]\n",
    "\n",
    "plot_class_regions_for_classifier(clf, X_twovar_test, y_test)\n",
    "plt.title(\"SVC, balanceado, para acurácia\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([0.0, 1.01])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.title (\"Curva de Precisão-recall: SVC, balanceado\")\n",
    "plt.plot(precision, recall, label = 'Curva de Precisão-recall')\n",
    "plt.plot(closest_zero_p, closest_zero_r, 'o', markersize=12, fillstyle='none', c='r', mew=3)\n",
    "plt.xlabel('Precisão', fontsize=16)\n",
    "plt.ylabel('Recall', fontsize=16)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()\n",
    "print('No zero, precisão: {:.2f}, recall: {:.2f}'\n",
    "      .format(closest_zero_p, closest_zero_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificadores Naive Bayes\n",
    "\n",
    "#### 7 - Exemplo de classificador Gaussiano Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state=0)\n",
    "\n",
    "nbclf = GaussianNB().fit(X_train, y_train)\n",
    "plot_class_regions_for_classifier(nbclf, X_train, y_train, X_test, y_test,\n",
    "                                 'Classificador Gausseano Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 - Exemplo de classificador Gaussiano Naive Bayes em uma base de dados complexa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2,\n",
    "                                                   random_state=0)\n",
    "\n",
    "nbclf = GaussianNB().fit(X_train, y_train)\n",
    "plot_class_regions_for_classifier(nbclf, X_train, y_train, X_test, y_test,\n",
    "                                 'Classificador Gausseano Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjuntos de árvores de decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 8 - Florestas Aleatórias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2,\n",
    "                                                   random_state = 0)\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "title = 'Floresta Aleatória'\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train, X_test,\n",
    "                                         y_test, title, subaxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9 - Floresta aleatória na base de dados frutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits.as_matrix(),\n",
    "                                                   y_fruits.as_matrix(),\n",
    "                                                   random_state = 0)\n",
    "fig, subaxes = plt.subplots(6, 1, figsize=(6, 32))\n",
    "\n",
    "title = 'Floresta Aleatória'\n",
    "pair_list = [[0,1], [0,2], [0,3], [1,2], [1,3], [2,3]]\n",
    "\n",
    "for pair, axis in zip(pair_list, subaxes):\n",
    "    X = X_train[:, pair]\n",
    "    y = y_train\n",
    "    \n",
    "    clf = RandomForestClassifier(n_jobs=8, max_depth = 2).fit(X, y)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X, y, None,\n",
    "                                             None, title, axis,\n",
    "                                             target_names_fruits)\n",
    "    \n",
    "    axis.set_xlabel(feature_names_fruits[pair[0]])\n",
    "    axis.set_ylabel(feature_names_fruits[pair[1]])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 10,\n",
    "                            random_state=0).fit(X_train, y_train)\n",
    "\n",
    "print('Acurácia no treinamento: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Acurácia no teste: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 - Árvores de decisão com o gradiente aumentado (Gradient-boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "title = 'Classificador Gradiente Aumentado'\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train, X_test,\n",
    "                                         y_test, title, subaxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11 - Árvores com gradiente aumentado no dataset de frutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits.as_matrix(),\n",
    "                                                   y_fruits.as_matrix(),\n",
    "                                                   random_state = 0)\n",
    "fig, subaxes = plt.subplots(6, 1, figsize=(6, 32))\n",
    "\n",
    "clf = GradientBoostingClassifier?\n",
    "\n",
    "pair_list = [[0,1], [0,2], [0,3], [1,2], [1,3], [2,3]]\n",
    "\n",
    "for pair, axis in zip(pair_list, subaxes):\n",
    "    X = X_train[:, pair]\n",
    "    y = y_train\n",
    "    \n",
    "    clf = GradientBoostingClassifier().fit(X, y)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X, y, None,\n",
    "                                             None, title, axis,\n",
    "                                             target_names_fruits)\n",
    "    \n",
    "    axis.set_xlabel(feature_names_fruits[pair[0]])\n",
    "    axis.set_ylabel(feature_names_fruits[pair[1]])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "\n",
    "print('Acurácia no treinamento: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Acurácia no teste: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos lineares para classificação multi-classe\n",
    "\n",
    "#### 12 - Função LinearSVC (Um contra todos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits_2d, y_fruits_2d, random_state = 0)\n",
    "\n",
    "clf = LinearSVC(C=5, random_state = 67).fit(X_train, y_train)\n",
    "print('Coeficientes:\\n', clf.coef_)\n",
    "print('Interceptos:\\n', clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13 - Plot dos resultados multi-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "colors = ['r', 'g', 'b', 'y']\n",
    "cmap_fruits = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#FFFF00'])\n",
    "\n",
    "plt.scatter(X_fruits_2d[['height']], X_fruits_2d[['width']],\n",
    "           cmap=cmap_fruits, edgecolor = 'black', alpha=.7)\n",
    "\n",
    "# falta c=y_fruits_2d\n",
    "\n",
    "x_0_range = np.linspace(-10, 15)\n",
    "\n",
    "for w, b, color in zip(clf.coef_, clf.intercept_, ['r', 'g', 'b', 'y']):\n",
    "    plt.plot(x_0_range, -(x_0_range * w[0] + b) / w[1], alpha=.8)\n",
    "    \n",
    "plt.legend(target_names_fruits)\n",
    "plt.xlabel('height')\n",
    "plt.ylabel('width')\n",
    "plt.xlim(-2, 12)\n",
    "plt.ylim(-2, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação para classificação multi-classe\n",
    "\n",
    "#### 14 - Matriz de confusão multi-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target\n",
    "X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(X, y, random_state=0)\n",
    "\n",
    "\n",
    "svm = SVC(kernel = 'linear').fit(X_train_mc, y_train_mc)\n",
    "svm_predicted_mc = svm.predict(X_test_mc)\n",
    "confusion_mc = confusion_matrix(y_test_mc, svm_predicted_mc)\n",
    "df_cm = pd.DataFrame(confusion_mc, \n",
    "                     index = [i for i in range(0,10)], columns = [i for i in range(0,10)])\n",
    "\n",
    "plt.figure(figsize=(5.5,4))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('SVM Kernel Linear\\nAcurácia:{0:.3f}'.format(accuracy_score(y_test_mc, \n",
    "                                                                       svm_predicted_mc)))\n",
    "plt.ylabel('Verdadeiro')\n",
    "plt.xlabel('Previsto')\n",
    "\n",
    "\n",
    "svm = SVC(kernel = 'rbf').fit(X_train_mc, y_train_mc)\n",
    "svm_predicted_mc = svm.predict(X_test_mc)\n",
    "confusion_mc = confusion_matrix(y_test_mc, svm_predicted_mc)\n",
    "df_cm = pd.DataFrame(confusion_mc, index = [i for i in range(0,10)],\n",
    "                  columns = [i for i in range(0,10)])\n",
    "\n",
    "plt.figure(figsize = (5.5,4))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('SVM Kernel RBF\\nAcurácia:{0:.3f}'.format(accuracy_score(y_test_mc, \n",
    "                                                                    svm_predicted_mc)))\n",
    "plt.ylabel('Verdadeiro')\n",
    "plt.xlabel('Previsto');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15 - Reporte de classificação multi-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test_mc, svm_predicted_mc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16 - Micro vs. Macro metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precisão Micro-média = {:.2f} (trata as instâncias igualmente)'\n",
    "      .format(precision_score(y_test_mc, svm_predicted_mc, average = 'micro')))\n",
    "print('Precisão Macro-média = {:.2f} (trata as classes igualmente)'\n",
    "      .format(precision_score(y_test_mc, svm_predicted_mc, average = 'macro')))\n",
    "\n",
    "\n",
    "print('Micro-média f1 = {:.2f} (trata as instâncias igualmente)'\n",
    "      .format(f1_score(y_test_mc, svm_predicted_mc, average = 'micro')))\n",
    "print('Macro-média f1 = {:.2f} (trata as classes igualmente)'\n",
    "      .format(f1_score(y_test_mc, svm_predicted_mc, average = 'macro')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métrica para Regressão\n",
    "\n",
    "#### 17 - Exemplo com a função LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "X = diabetes.data[:, None, 6]\n",
    "y = diabetes.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "lm = LinearRegression().fit(X_train, y_train)\n",
    "lm_dummy_mean = DummyRegressor(strategy = 'mean').fit(X_train, y_train)\n",
    "\n",
    "y_predict = lm.predict(X_test)\n",
    "y_predict_dummy_mean = lm_dummy_mean.predict(X_test)\n",
    "\n",
    "print('Coeficientes: ', lm.coef_)\n",
    "print(\"Erro quadrado médio (dummy): {:.2f}\".format(mean_squared_error(y_test, \n",
    "                                                                     y_predict_dummy_mean)))\n",
    "print(\"Erro quadrado médio (linear model): {:.2f}\".format(mean_squared_error(y_test, y_predict)))\n",
    "print(\"Escore r2 (dummy): {:.2f}\".format(r2_score(y_test, y_predict_dummy_mean)))\n",
    "print(\"Escore r2 (linear model): {:.2f}\".format(r2_score(y_test, y_predict)))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(X_test, y_test,  color='black')\n",
    "plt.plot(X_test, y_predict, color='green', linewidth=2)\n",
    "plt.plot(X_test, y_predict_dummy_mean, color='red', linestyle = 'dashed', \n",
    "         linewidth=2, label = 'dummy')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
